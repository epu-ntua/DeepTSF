version: '3.4'
# to run this you need to create a .env file with the contents of .env.example
services:
  # base:
  #   container_name: deeptsf_base
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.base
  #   image: conda-base:latest
  #   # We don't need to run anything for the base image. Just build it.
  #   entrypoint: /bin/true

  s3:
    # image: minio/minio:RELEASE.2024-03-30T09-41-56Z
    # image: minio/minio:RELEASE.2020-12-18T03-27-42Z
    image: minio/minio:RELEASE.2025-02-18T16-25-55Z
    container_name: s3
    restart: always
    hostname: s3
    volumes:
      - artifact-store:/data
    ports:
      - 9000:9000
      - 9001:9001
    networks:
      - load-forecasting-net
    command:
        server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 10s
      retries: 3
    env_file:
      - .env
      # Uses following .env parameters:
      # MINIO_ROOT_USER
      # MINIO_ROOT_PASSWORD


  create_bucket:
    # image: minio/mc:RELEASE.2019-07-17T22-13-42Z
    # image: minio/minio:RELEASE.2024-03-30T09-41-56Z
    image: minio/minio:RELEASE.2025-02-18T16-25-55Z
    depends_on:
      - s3    
    networks:
       - load-forecasting-net
    entrypoint: >
      /bin/sh -c '
      sleep 5;
      /usr/bin/mc config host add s3 http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} --api S3v4;
      [[ ! -z "`/usr/bin/mc ls s3 | grep challenge`" ]] || /usr/bin/mc mb s3/${BUCKET_NAME};
      /usr/bin/mc policy download s3/${BUCKET_NAME};
      /usr/bin/mc policy set public s3/${BUCKET_NAME};
      exit 0;
      '
    env_file:
      - .env
      # Uses following .env parameters:
      # MINIO_ROOT_USER
      # MINIO_ROOT_PASSWORD


  pgdb:
    container_name: pgdb
    hostname: pgdb
    restart: always
    image: inergy2020iccs/pgdb:16.2
    volumes:
        - database:/var/lib/postgresql/data
    ports:
      - 5432:5432
    expose:
      - '5432'
    environment:
      PGDATA: /var/lib/postgresql/data/pgdata
    networks:
      - load-forecasting-net
    env_file:
      - .env
      # Uses following .env parameters:
      # POSTGRES_USER
      # POSTGRES_PASSWORD
      # POSTGRES_DATABASE


  mlflow_server:
    container_name: mlflow_server
    hostname: mlflow_server
    restart: always
    image: inergy2020iccs/mlflow_server:2.11.3
    ports:
      - 5000:5000
    expose:
      - 5000
    depends_on: 
      - s3
      - pgdb
    networks:
      - load-forecasting-net
    command: bash -c 
      "mlflow server --backend-store-uri postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@pgdb:5432/${POSTGRES_DATABASE} --host 0.0.0.0 --default-artifact-root s3://${BUCKET_NAME}/"
    env_file:
      - .env
      # Uses following .env parameters:
      # POSTGRES_PASSWORD
      # POSTGRES_USER
      # POSTGRES_DATABASE
      # MLFLOW_S3_ENDPOINT_URL
      # AWS_ACCESS_KEY_ID
      # AWS_SECRET_ACCESS_KEY


  backend:
    hostname: deeptsf_backend
    container_name: deeptsf_backend
    # image: inergy2020iccs/deeptsf_backend:1.0.2  # only for releases
    build: .    
    # restart: always
    ports:
      - '8080:8080'
    expose:
      - '8080'
    # volumes:
      # - .:/app/. #debugging only
    stdin_open: 
      true
    env_file:
      - .env
    # volumes:
    #   - ./shared:/shared
    depends_on:
      - redis
      # - base
    # Uses all .env parameters
    # deploy: # activate this if you want gpu usage
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           capabilities: [gpu]
    networks:
       - load-forecasting-net

  worker:
    build:
      dockerfile: ./celery_DeepTSF/Dockerfile
      context: .
    env_file:
      - .env
    # depends_on:
      # - base
    # volumes:
    #   - ./shared:/shared:ro
    networks:
       - load-forecasting-net

  redis:
    image: redis:7-alpine
    networks:
       - load-forecasting-net
       
  dagster:
    container_name: deeptsf_dagster
    hostname: deeptsf_dagster
    # image: inergy2020iccs/deeptsf_dagster:1.0.1 # only for releases
    build:
      context: .
      dockerfile: ./dagster_deeptsf/config/Dockerfile
    # deploy: # activate this if you want gpu usage
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           capabilities: [gpu]
    expose:
      - 8006
    ports:
      - 8006:8006
    # depends_on:
      # - base
    # volumes:
    #   - dagster-deeptsf-logs:/app/dagster_deeptsf
    #   - ./uc2/user_datasets:/app/dagster_deeptsf/user_datasets
      # - ./dagster-deeptsf/dagster_deeptsf:/app/dagster_deeptsf/dagster_deeptsf  #debugging only
    networks:
      - load-forecasting-net
    env_file:
      - .env
      # Uses following .env parameters:
      # MLFLOW_TRACKING_URI
      # MLFLOW_S3_ENDPOINT_URL
      # AWS_ACCESS_KEY_ID
      # AWS_SECRET_ACCESS_KEY

  postgres:
    image: inergy2020iccs/pgdb:16.2
    volumes:
        - database2:/var/lib/postgresql/data
    restart: always
    container_name: postgres
    hostname: postgres
    ports:
      - '5430:5430'
    command: ["postgres", "-c", "port=5430"]
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: dagster_celery
    networks:
      - load-forecasting-net

  frontend:
    container_name: deeptsf_dashboard
    hostname: deeptsf_dashboard
    image: inergy2020iccs/deeptsf_dashboard:1.0.2
    # build: https://github.com/I-NERGY/deeptsf-dashboard.git
    restart: always
    expose:
      - 3000
    ports:
      - 3000:3000
    stdin_open: true
    # environment:
    ## These environment variable cannot be set from within here. (React issues)
    ## Instead they should have been included as .env in the front end repo / image.
    #   DAGSTER_ENDPOINT_URL: ${DAGSTER_ENDPOINT_URL}
    #   REACT_APP_AUTH: "false"
    #   REACT_APP_MLFLOW: ${MLFLOW_TRACKING_URI}
    #   CHOKIDAR_USEPOLLING: "true"
    #   REACT_APP_NAME: Load_Forecasting_Dashboard
    networks:
      - load-forecasting-net

volumes:
  database:
  database2:
  artifact-store:
  # dagster-deeptsf-logs:

networks: 
  load-forecasting-net: